########################
# zeek -> arkime session creation and enrichment
#
# see https://docs.zeek.org/en/stable/script-reference/log-files.html for Zeek logfile documentation
#
# see source.zeeklogs.js for the Arkime code that turns these into UI fields
#
# to profile, debug:
#   - get filters sorted by execution time (where in > 0)
#   $ docker compose exec logstash curl -XGET http://localhost:9600/_node/stats/pipelines | jq -r '.. | .filters? // empty | .[] | objects | select (.events.in > 0) | [.id, .events.in, .events.out, .events.duration_in_millis] | join (";")' | sort -n -t ';' -k4
#   - get filters where in != out
#   $ docker compose exec logstash curl -XGET http://localhost:9600/_node/stats/pipelines | jq -r '.. | .filters? // empty | .[] | objects | select (.events.in != .events.out) | [.id, .events.in, .events.out, .events.duration_in_millis] | join (";")'
#
# Copyright (c) 2025 Battelle Energy Alliance, LLC.  All rights reserved.
#######################

filter {

  # handle JSON-formatted Zeek logs right out of the gate, we'll do the field renaming below
  if ([message] =~ /^{.*}$/) { json {
    id => "json_zeek_message_parse"
    source => "[message]"
    target => "[zeek_cols]"
    add_tag => [ "_jsonparsesuccess" ]
  } }


  # in JSON, do some global renaming of common fields to make them match the names we'd
  #   be assigning to them if we were reading TSV
  if ("_jsonparsesuccess" in [tags]) {

    # some of the ICSNPP parsers do an interesting thing to handle source and destination fields
    #   (see https://github.com/cisagov/icsnpp-bacnet/?tab=readme-ov-file#source-and-destination-fields)
    #   so check for and handle those first
    ruby {
      id => "ruby_zeek_json_determine_source_destination_fields"
      code => "
        if ![event.get('[zeek_cols][source_h]').to_s,
             event.get('[zeek_cols][source_p]').to_s,
             event.get('[zeek_cols][destination_h]').to_s,
             event.get('[zeek_cols][destination_p]').to_s].reject{ |e| e.nil? || e.empty? || (e == '0') }.empty? then
          event.set('[@metadata][icsnpp_source_dest_fields]', 'true')
        end
      "
    }
    if ([@metadata][icsnpp_source_dest_fields]) {
      mutate {
        id => "mutate_rename_zeek_json_common_reversed_direction_fields"
        rename => { "[zeek_cols][id.orig_h]" => "[zeek_cols][drop_orig_h]" }
        rename => { "[zeek_cols][id.orig_p]" => "[zeek_cols][drop_orig_p]" }
        rename => { "[zeek_cols][id.resp_h]" => "[zeek_cols][drop_resp_h]" }
        rename => { "[zeek_cols][id.resp_p]" => "[zeek_cols][drop_resp_p]" }
        rename => { "[zeek_cols][source_h]" => "[zeek_cols][orig_h]" }
        rename => { "[zeek_cols][source_p]" => "[zeek_cols][orig_p]" }
        rename => { "[zeek_cols][destination_h]" => "[zeek_cols][resp_h]" }
        rename => { "[zeek_cols][destination_p]" => "[zeek_cols][resp_p]" }
      }

    } else {
      mutate {
        id => "mutate_rename_zeek_json_common_fields"
        rename => { "[zeek_cols][id.orig_h]" => "[zeek_cols][orig_h]" }
        rename => { "[zeek_cols][id.orig_p]" => "[zeek_cols][orig_p]" }
        rename => { "[zeek_cols][id.resp_h]" => "[zeek_cols][resp_h]" }
        rename => { "[zeek_cols][id.resp_p]" => "[zeek_cols][resp_p]" }
      }
    } # icsnpp_source_dest_fields or not
  } # _jsonparsesuccess in tags

  # for non-JSON:
  # The Dissect is WAY faster than CSV, and quite a bit faster than mutate.split. However, it
  # is not as flexible when it comes to missing or extra columns
  # (See https://github.com/logstash-plugins/logstash-filter-dissect/issues/62)
  #
  # So, if the dissect filter fails, we're going to fall back to split-then-zip solution.
  # This should be a good tradeoff between performance (in the case where the Zeek logs
  # match what we think they should look like) and flexibility (when they don't).
  #
  # The one drawback is that if you make a change to the fields in dissect, make sure
  # you make the corresponding change in the ruby init code.

} # end Filter
